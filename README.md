This repository is focused on the end-to-end process of building a Large Language Model (LLM) from scratch. To date, we have successfully completed the initial data preprocessing pipeline, a fundamental step in preparing data for model training. The steps completed so far include:

1.   Creating the input dataset: We start by gathering and structuring the text data needed for training the LLM. This includes ensuring the dataset is clean and formatted in a way that it can be effectively tokenized and used for model training.
2.   Converting the text into tokens: Once the dataset is ready, the next step involves breaking down the raw text into smaller units called tokens. Tokenization is crucial because it converts the text into discrete elements that the model can process.
3.   Transforming tokens into token IDs: After tokenization, the tokens are mapped to unique numerical representations, or token IDs. This step is necessary because machine learning models typically operate on numeric data, not raw text.
4.   Generating token embeddings, with a focus on positional embeddings: The final step completed so far is the conversion of token IDs into token embeddings, particularly positional embeddings. Positional embeddings help the model understand the order of tokens in a sequence, allowing it to capture the context of words based on their position in the text.
5.   Implementing a simplified attention mechanism (without trainable weights): We have introduced a fundamental attention mechanism that focuses purely on computing attention scores, attention weights, and context vectors without involving trainable parameters. This serves as the foundation for understanding more advanced attention-based architectures, such as Transformers.

These preprocessing steps are critical for building a solid foundation for the LLM, ensuring the data is properly structured and ready for the next stages of model architecture development, training, and fine-tuning. Further components, including self-attention, multi-head attention, and transformer-based model training, will be added in the upcoming phases of the project.
